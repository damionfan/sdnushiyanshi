#include "../incldue/active.h"

__global__ void ReluForward(const float n, const unsigned float *in, const unsigned float *out, const unsigned float negative_slope){
	for(int i = blockIdx.x * blockDim.x + threadIdx.x;i<n;i+=blockIdx.x*dlockDim.x){
		out[i] = in[i]>0 ? in[i]:in[i]*negative_slope;
	}
}

void ReluLayer_Forward(const unsigned char *in,const unsigned char *out){
	unsigned float *data_gpu,*out_gpu;
	unsigned float *n_gpu,*negative_slope;
	unsigned float n = sizeof(in)/sieof(in[0]);

	cudaMalloc((void**)&data_gpu,n*sizeof(float));
	cudaMalloc((void**)&out_gpu,n*sizeof(float));
	cudaMalloc((void**)&n_gpu,sizeof(float));
	cudaMalloc((void**)&negative_slope,sizeof(float));




}
